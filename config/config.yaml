# ONDEWO T2S batch server configuration


inference:
    type: "testme1"

    composite_inference:
        text2mel:
            type: "glow_tts"

            tacotron2:
                batch_size: 8
                path: "models/tacotron2/de/checkpoints_kerstin_eloqai/"
                param_config_path: "models/tacotron2/de/tacotron2.yaml"
                step: "1000"

            glow_tts:
                batch_size: 8
                use_gpu: false
                length_scale: 1.0
                noise_scale: 0.667
                batch_inference: false
                path: "models/glow-tts/de/G_florian.pth"
                param_config_path: "models/glow-tts/de/config.json"

        mel2audio:
            type: "mb_melgan_triton"

            waveglow:
                batch_size: 4
                path: "models/waveglow/checkpoints_waveglow_florian/WaveGlowNM-STEP-3000.pt"
                param_config_path: "models/waveglow/waveglow.yaml"
                sigma: 0.6
                denoiser:
                    active: true
                    strength: 0.05

            waveglow_triton:
                param_config_path: "models/waveglow/waveglow.yaml"
                sigma: 0.6
                max_spect_size: 1000
                triton_model_name: "waveglow"
                triton_url: "localhost:8001"

            mb_melgan_tf:
                batch_size: 20
                config_path: "models/mb_melgan/en/config.yml"
                model_path: "models/mb_melgan/en/generator-1820000.h5"
                stats_path: "models/mb_melgan/en/stats.npy"

            mb_melgan_triton:
                config_path: "models/mb_melgan/en/config.yml"
                stats_path: "models/mb_melgan/en/stats.npy"
                triton_model_name: "mb_melgan"
                triton_url: "localhost:8001"

    caching:
        active: false
        memory_cache_max_size: 20
        sampling_rate: 22050
        load_cache: true
        save_cache: true
        cache_save_dir: "models/cache"
